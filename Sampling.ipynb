{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66786819-a499-4b84-b050-7c45ce60f1db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Downloading imbalanced_learn-0.13.0-py3-none-any.whl.metadata (8.8 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.14.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.5.2)\n",
      "Collecting sklearn-compat<1,>=0.1 (from imbalanced-learn)\n",
      "  Downloading sklearn_compat-0.1.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\hp\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Downloading imbalanced_learn-0.13.0-py3-none-any.whl (238 kB)\n",
      "Downloading sklearn_compat-0.1.3-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: sklearn-compat, imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.13.0 sklearn-compat-0.1.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install imbalanced-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba23867b-fa3c-4e9b-a214-95fa2958f3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before SMOTE: Counter({0: 763, 1: 9})\n",
      "After SMOTE: Counter({0: 763, 1: 763})\n",
      "Sampling1:\n",
      "  M1: 0.42\n",
      "  M2: 0.52\n",
      "  M3: 0.49\n",
      "  M4: 0.47\n",
      "  M5: 0.54\n",
      "Sampling2:\n",
      "  M1: 0.91\n",
      "  M2: 0.99\n",
      "  M3: 0.98\n",
      "  M4: 0.96\n",
      "  M5: 0.97\n",
      "Sampling3:\n",
      "  M1: 0.92\n",
      "  M2: 0.99\n",
      "  M3: 0.99\n",
      "  M4: 0.96\n",
      "  M5: 0.99\n",
      "Sampling4:\n",
      "  M1: 0.49\n",
      "  M2: 0.42\n",
      "  M3: 0.48\n",
      "  M4: 0.51\n",
      "  M5: 0.49\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"https://raw.githubusercontent.com/AnjulaMehto/Sampling_Assignment/main/Creditcard_data.csv\"  # Path to your uploaded file\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Ensure target column 'Class' exists\n",
    "if \"Class\" not in data.columns:\n",
    "    raise KeyError(\"The dataset does not have a 'Class' column. Please verify the target variable.\")\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(\"Class\", axis=1)\n",
    "y = data[\"Class\"]\n",
    "\n",
    "# Balance the dataset using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "\n",
    "# Confirm the balancing\n",
    "print(f\"Before SMOTE: {Counter(y)}\")\n",
    "print(f\"After SMOTE: {Counter(y_resampled)}\")\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Define sampling techniques\n",
    "def random_sampling(data, n):\n",
    "    return data.sample(n, random_state=42).reset_index(drop=True)\n",
    "\n",
    "def systematic_sampling(data, n):\n",
    "    step = len(data) // n\n",
    "    indices = np.arange(0, len(data), step)[:n]\n",
    "    return data.iloc[indices].reset_index(drop=True)\n",
    "\n",
    "def stratified_sampling(X, y, n):\n",
    "    X_sample, _, y_sample, _ = train_test_split(X, y, test_size=1 - (n / len(y)), stratify=y, random_state=42)\n",
    "    return X_sample.reset_index(drop=True), y_sample.reset_index(drop=True)\n",
    "\n",
    "def cluster_sampling(data, clusters):\n",
    "    data = data.copy()\n",
    "    data['cluster'] = np.random.randint(0, clusters, len(data))\n",
    "    cluster_data = data[data['cluster'] == 0].drop(\"cluster\", axis=1)\n",
    "    return cluster_data.reset_index(drop=True)\n",
    "\n",
    "# Define sample size\n",
    "sample_size = 1000\n",
    "X_train_df = pd.DataFrame(X_train_scaled)\n",
    "y_train_df = pd.Series(y_train).reset_index(drop=True)\n",
    "\n",
    "# Generate samples\n",
    "random_sample = random_sampling(X_train_df, sample_size)\n",
    "systematic_sample = systematic_sampling(X_train_df, sample_size)\n",
    "stratified_sample_X, stratified_sample_y = stratified_sampling(X_train_df, y_train_df, sample_size)\n",
    "cluster_sample = cluster_sampling(X_train_df, clusters=10)\n",
    "\n",
    "# Define models\n",
    "models = {\n",
    "    \"M1\": LogisticRegression(max_iter=5000),\n",
    "    \"M2\": RandomForestClassifier(),\n",
    "    \"M3\": GradientBoostingClassifier(),\n",
    "    \"M4\": SVC(),\n",
    "    \"M5\": DecisionTreeClassifier(),\n",
    "}\n",
    "\n",
    "# Apply models on samples\n",
    "results = {}\n",
    "samples = [\n",
    "    (random_sample, y_train_df.iloc[random_sample.index]),\n",
    "    (systematic_sample, y_train_df.iloc[systematic_sample.index]),\n",
    "    (stratified_sample_X, stratified_sample_y),\n",
    "    (cluster_sample, y_train_df.iloc[cluster_sample.index]),\n",
    "]\n",
    "\n",
    "for i, (sample, sample_y) in enumerate(samples):\n",
    "    results[f\"Sampling{i+1}\"] = {}\n",
    "    for model_name, model in models.items():\n",
    "        model.fit(sample, sample_y)  # Fit model on the sample\n",
    "        accuracy = model.score(X_test_scaled, y_test)\n",
    "        results[f\"Sampling{i+1}\"][model_name] = accuracy\n",
    "\n",
    "# Print results\n",
    "for sampling, model_results in results.items():\n",
    "    print(f\"{sampling}:\")\n",
    "    for model, acc in model_results.items():\n",
    "        print(f\"  {model}: {acc:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
